{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hj2222222/API/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "gNwZd2lrdy9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSdlvhyJ_rh4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "stock_data = pd.read_csv('/content/drive/MyDrive/산업 AI 캡스톤/DATA/Preprocessing_Data/Preprocessing_Neumeric_Data/neumeric_preprocess_kospi.csv')\n",
        "stock_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_numeric_data(df):\n",
        "\n",
        "    # 데이터들 소수점 3자리까지 조절\n",
        "    df = df.round(3)\n",
        "\n",
        "    # 종가들을 제외하고 스케일링을 진행하기위해서 우선 칼럼들의 이름을 뽑아내기\n",
        "    df_names = df.columns.tolist()\n",
        "\n",
        "    # 종가 열(Kospi_close, Kosdaq_close)을 데이터프레임에서 제외\n",
        "    col_to_drop = [col for col in df.columns if 'Kospi_close' in col or 'Kosdaq_close' in col]\n",
        "    df_col = df.columns.tolist()\n",
        "    # 날짜 열(Date)을 데이터프레임에서 제외\n",
        "    df_col.remove('Date')\n",
        "    x_col = [item for item in df_col if item not in col_to_drop]\n",
        "\n",
        "    # MinMax Scaling 적용\n",
        "    columns_to_scale = x_col  # 스케일링을 적용할 열 목록\n",
        "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "FZO3-ZNud_9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "data = stock_data['Kospi_close'].values.reshape(-1, 1)\n",
        "# stock_data = preprocess_numeric_data(stock_data)\n",
        "stock_data = stock_data.drop(['Kospi_close'],axis=1)\n",
        "stock_data = stock_data.drop(['Date'],axis=1)\n",
        "# scaler = MinMaxScaler()\n",
        "# data = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "A3STevXvd3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = [stock_data.columns]\n",
        "\n",
        "close = ['Kospi_open', 'Kospi_high', 'Kospi_low', 'Kospi_vol', 'dji_open',\n",
        "       'dji_high', 'dji_low', 'dji_vol', 'us500_open',\n",
        "       'us500_high', 'us500_low', 'us500_vol', 'ex_AM_open',\n",
        "       'ex_AM_high', 'ex_AM_low', 'ex_JP_open', 'ex_JP_high',\n",
        "       'ex_JP_low', 'ko_interest', 'ko_consumer',\n",
        "       'ko_real_estate']\n",
        "\n",
        "close_only = ['Kospi_open', 'Kospi_high', 'Kospi_low', 'Kospi_vol', 'dji_open',\n",
        "       'dji_high', 'dji_low','dji_vol', 'us500_open',\n",
        "       'us500_high', 'us500_low', 'us500_vol', 'ex_AM_open',\n",
        "       'ex_AM_high', 'ex_AM_low', 'ex_JP_open', 'ex_JP_high',\n",
        "       'ex_JP_low']\n",
        "\n",
        "stock_data_close = stock_data.drop(close, axis=1)\n",
        "stock_data_close_only = stock_data.drop(close_only, axis=1)"
      ],
      "metadata": {
        "id": "PiURLoOMYT9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_close_only.tail(5)"
      ],
      "metadata": {
        "id": "OCibTUqAdd2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분할\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "train_data_t, test_data_t = stock_data[:train_size], stock_data[train_size:]\n",
        "\n",
        "def create_dataset(dataset, data, look_back=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(dataset[i:(i + look_back)])\n",
        "        y.append(data[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "look_back = 10  # 몇 일 동안의 데이터를 사용할 것인지 설정\n",
        "\n",
        "trainX, trainY = create_dataset(train_data_t, train_data, look_back)\n",
        "testX, testY = create_dataset(test_data_t,test_data, look_back)\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)"
      ],
      "metadata": {
        "id": "-Z668UWokbbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlystopping = EarlyStopping(monitor='val_loss', patience=20)"
      ],
      "metadata": {
        "id": "bXsrzbaWQsKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Sequential()\n",
        "# model.add(Bidirectional(LSTM(units=1024, activation='relu', return_sequences=True), input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(GRU(units=1024, activation='relu', return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GRU(units=512, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GRU(units=256, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GRU(units=128, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GRU(units=64, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GRU(units=32, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "# model.add(GRU(units=16, activation='relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(units=1))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(trainX, trainY, epochs=300, batch_size=64,\n",
        "          validation_split=0.1, verbose=2)            #callbacks=[early_stopping]\n",
        "model.save_weights('./lstm_weights.h1')\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 예측\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# 예측 결과 역 스케일링\n",
        "# trainPredict = trainPredict.detach().cpu().numpy()\n",
        "# trainPredict = scaler.inverse_transform(trainPredict)\n",
        "# trainY = trainY.detach().cpu().numpy()\n",
        "# trainY = scaler.inverse_transform([trainY])\n",
        "# testPredict = testPredict.detach().cpu().numpy()\n",
        "# testPredict = scaler.inverse_transform(testPredict)\n",
        "# testY = testY.detach().cpu().numpy()\n",
        "# testY = scaler.inverse_transform([testY])\n",
        "\n",
        "# 예측 시각화\n",
        "plt.plot(trainY, label='Actual Price')\n",
        "plt.plot(trainPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Training Data')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(testY, label='Actual Price')\n",
        "plt.plot(testPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Testing Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yzlIN0Yze3y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터 분할\n",
        "# train_size = int(len(data) * 0.8)\n",
        "# train_data, test_data = data[:train_size], data[train_size:]\n",
        "# train_data_t, test_data_t = stock_data_close[:train_size], stock_data_close[train_size:]\n",
        "\n",
        "# def create_dataset(dataset, data, look_back=10):\n",
        "#     X, y = [], []\n",
        "#     for i in range(len(data) - look_back):\n",
        "#         X.append(dataset[i:(i + look_back)])\n",
        "#         y.append(data[i + look_back])\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "# look_back = 10  # 몇 일 동안의 데이터를 사용할 것인지 설정\n",
        "\n",
        "# trainX, trainY = create_dataset(train_data_t, train_data, look_back)\n",
        "# testX, testY = create_dataset(test_data_t,test_data, look_back)\n",
        "# print(trainX.shape)\n",
        "# print(trainY.shape)"
      ],
      "metadata": {
        "id": "7GXh34sht6Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Sequential()\n",
        "# model.add(GRU(units=1024, activation='relu', return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(GRU(units=512, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(units=256, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(units=128, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(units=64, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(units=32, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(units=16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "loss = Huber()\n",
        "optimizer = Adam(0.0005)\n",
        "model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
        "checkpoint = ModelCheckpoint(filename,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(trainX, trainY, epochs=300, batch_size=64,\n",
        "          validation_split=0.1, verbose=2,callbacks=[checkpoint, earlystopping])\n",
        "\n",
        "model.save_weights('./lstm_weights.h2')\n",
        "\n",
        "model.load_weights(filename)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 예측\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# 예측 결과 역 스케일링\n",
        "# trainPredict = trainPredict.detach().cpu().numpy()\n",
        "# trainPredict = scaler.inverse_transform(trainPredict)\n",
        "# trainY = trainY.detach().cpu().numpy()\n",
        "# trainY = scaler.inverse_transform([trainY])\n",
        "# testPredict = testPredict.detach().cpu().numpy()\n",
        "# testPredict = scaler.inverse_transform(testPredict)\n",
        "# testY = testY.detach().cpu().numpy()\n",
        "# testY = scaler.inverse_transform([testY])\n",
        "\n",
        "# 예측 시각화\n",
        "plt.plot(trainY, label='Actual Price')\n",
        "plt.plot(trainPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Training Data')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(testY, label='Actual Price')\n",
        "plt.plot(testPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Testing Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fzmVz1OUd_Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터 분할\n",
        "# train_size = int(len(data) * 0.8)\n",
        "# train_data, test_data = data[:train_size], data[train_size:]\n",
        "# train_data_t, test_data_t = stock_data_close_only[:train_size], stock_data_close_only[train_size:]\n",
        "\n",
        "# def create_dataset(dataset, data, look_back=10):\n",
        "#     X, y = [], []\n",
        "#     for i in range(len(data) - look_back):\n",
        "#         X.append(dataset[i:(i + look_back)])\n",
        "#         y.append(data[i + look_back])\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "# look_back = 10  # 몇 일 동안의 데이터를 사용할 것인지 설정\n",
        "\n",
        "# trainX, trainY = create_dataset(train_data_t, train_data, look_back)\n",
        "# testX, testY = create_dataset(test_data_t,test_data, look_back)\n",
        "# print(trainX.shape)\n",
        "# print(trainY.shape)"
      ],
      "metadata": {
        "id": "LwwdPxr9dpSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(units=1024, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=512, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=256, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=128, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=64, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=32, activation='relu',return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(units=16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(units=1))\n",
        "model.summary()\n",
        "\n",
        "loss = Huber()\n",
        "optimizer = Adam(0.0005)\n",
        "model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
        "checkpoint = ModelCheckpoint(filename,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(trainX, trainY, epochs=300, batch_size=64,\n",
        "          validation_split=0.1, verbose=2,callbacks=[checkpoint, earlystopping])\n",
        "\n",
        "model.save_weights('./lstm_weights.h3')\n",
        "\n",
        "model.load_weights(filename)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 예측\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# 예측 결과 역 스케일링\n",
        "# trainPredict = trainPredict.detach().cpu().numpy()\n",
        "# trainPredict = scaler.inverse_transform(trainPredict)\n",
        "# trainY = trainY.detach().cpu().numpy()\n",
        "# trainY = scaler.inverse_transform([trainY])\n",
        "# testPredict = testPredict.detach().cpu().numpy()\n",
        "# testPredict = scaler.inverse_transform(testPredict)\n",
        "# testY = testY.detach().cpu().numpy()\n",
        "# testY = scaler.inverse_transform([testY])\n",
        "\n",
        "# 예측 시각화\n",
        "plt.plot(trainY, label='Actual Price')\n",
        "plt.plot(trainPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Training Data')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(testY, label='Actual Price')\n",
        "plt.plot(testPredict, label='Predicted Price')\n",
        "plt.legend()\n",
        "plt.title('Testing Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2RI8rro9drju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bad8895-c0aa-4785-d6df-897c531d92f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 8, 64)             7168      \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 4, 64)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 4, 1024)           4460544   \n",
            "                                                                 \n",
            " dropout_73 (Dropout)        (None, 4, 1024)           0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 4, 512)            3147776   \n",
            "                                                                 \n",
            " dropout_74 (Dropout)        (None, 4, 512)            0         \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 4, 256)            787456    \n",
            "                                                                 \n",
            " dropout_75 (Dropout)        (None, 4, 256)            0         \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 4, 128)            197120    \n",
            "                                                                 \n",
            " dropout_76 (Dropout)        (None, 4, 128)            0         \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 4, 64)             49408     \n",
            "                                                                 \n",
            " dropout_77 (Dropout)        (None, 4, 64)             0         \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 4, 32)             12416     \n",
            "                                                                 \n",
            " dropout_78 (Dropout)        (None, 4, 32)             0         \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_79 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8665041 (33.05 MB)\n",
            "Trainable params: 8665041 (33.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 7111.01709, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 21s - loss: 3917.0657 - mse: 22153830.0000 - val_loss: 7111.0171 - val_mse: 50577788.0000 - 21s/epoch - 2s/step\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 2: val_loss improved from 7111.01709 to 7103.85889, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 9s - loss: 4378.1846 - mse: 35630712.0000 - val_loss: 7103.8589 - val_mse: 50476040.0000 - 9s/epoch - 862ms/step\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 3: val_loss improved from 7103.85889 to 5828.77686, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 7s - loss: 4260.4863 - mse: 31159246.0000 - val_loss: 5828.7769 - val_mse: 33984568.0000 - 7s/epoch - 659ms/step\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 4: val_loss improved from 5828.77686 to 4426.23291, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 7s - loss: 4024.0737 - mse: 27754164.0000 - val_loss: 4426.2329 - val_mse: 19600052.0000 - 7s/epoch - 648ms/step\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 5: val_loss improved from 4426.23291 to 485.70355, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 7s - loss: 3861.9736 - mse: 27414276.0000 - val_loss: 485.7036 - val_mse: 281153.3750 - 7s/epoch - 663ms/step\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 6: val_loss did not improve from 485.70355\n",
            "11/11 - 6s - loss: 4325.0220 - mse: 41284536.0000 - val_loss: 8668.2510 - val_mse: 75152872.0000 - 6s/epoch - 574ms/step\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 7: val_loss did not improve from 485.70355\n",
            "11/11 - 7s - loss: 4047.8108 - mse: 31850564.0000 - val_loss: 2398.2461 - val_mse: 5758050.0000 - 7s/epoch - 681ms/step\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 8: val_loss did not improve from 485.70355\n",
            "11/11 - 6s - loss: 3825.6440 - mse: 34198996.0000 - val_loss: 3305.7930 - val_mse: 10935649.0000 - 6s/epoch - 539ms/step\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 9: val_loss did not improve from 485.70355\n",
            "11/11 - 8s - loss: 3571.6506 - mse: 23123220.0000 - val_loss: 2738.0127 - val_mse: 7503521.5000 - 8s/epoch - 700ms/step\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 10: val_loss did not improve from 485.70355\n",
            "11/11 - 6s - loss: 3039.3857 - mse: 14399908.0000 - val_loss: 2398.2314 - val_mse: 5757980.5000 - 6s/epoch - 546ms/step\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 11: val_loss did not improve from 485.70355\n",
            "11/11 - 8s - loss: 2748.9346 - mse: 11444010.0000 - val_loss: 2014.4557 - val_mse: 4064109.7500 - 8s/epoch - 707ms/step\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 12: val_loss did not improve from 485.70355\n",
            "11/11 - 6s - loss: 2867.7461 - mse: 13318008.0000 - val_loss: 2573.7834 - val_mse: 6633704.5000 - 6s/epoch - 554ms/step\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 13: val_loss did not improve from 485.70355\n",
            "11/11 - 8s - loss: 2873.1482 - mse: 12934193.0000 - val_loss: 3370.6648 - val_mse: 11368828.0000 - 8s/epoch - 687ms/step\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 14: val_loss did not improve from 485.70355\n",
            "11/11 - 6s - loss: 3113.4211 - mse: 15582291.0000 - val_loss: 1929.7740 - val_mse: 3730017.7500 - 6s/epoch - 556ms/step\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 15: val_loss improved from 485.70355 to 77.40735, saving model to tmp/ckeckpointer.ckpt\n",
            "11/11 - 8s - loss: 3293.1602 - mse: 18860464.0000 - val_loss: 77.4073 - val_mse: 8231.0830 - 8s/epoch - 726ms/step\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 16: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 2959.9600 - mse: 14162035.0000 - val_loss: 2434.1558 - val_mse: 5931616.0000 - 6s/epoch - 559ms/step\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 17: val_loss did not improve from 77.40735\n",
            "11/11 - 8s - loss: 2865.8123 - mse: 14530985.0000 - val_loss: 2081.3032 - val_mse: 4337967.5000 - 8s/epoch - 686ms/step\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 18: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 3396.0063 - mse: 23594390.0000 - val_loss: 3134.1553 - val_mse: 9830135.0000 - 6s/epoch - 557ms/step\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 19: val_loss did not improve from 77.40735\n",
            "11/11 - 7s - loss: 3903.7332 - mse: 36303176.0000 - val_loss: 2245.0029 - val_mse: 5046347.5000 - 7s/epoch - 647ms/step\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 20: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 3659.4646 - mse: 28686222.0000 - val_loss: 580.0380 - val_mse: 341069.0938 - 6s/epoch - 574ms/step\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 21: val_loss did not improve from 77.40735\n",
            "11/11 - 7s - loss: 4402.7993 - mse: 38398320.0000 - val_loss: 4300.7378 - val_mse: 18504736.0000 - 7s/epoch - 592ms/step\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 22: val_loss did not improve from 77.40735\n",
            "11/11 - 7s - loss: 4496.9517 - mse: 41294056.0000 - val_loss: 5225.9756 - val_mse: 27638842.0000 - 7s/epoch - 650ms/step\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 23: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 3880.9138 - mse: 33055034.0000 - val_loss: 737.0790 - val_mse: 548053.0000 - 6s/epoch - 571ms/step\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 24: val_loss did not improve from 77.40735\n",
            "11/11 - 7s - loss: 3796.4519 - mse: 33999528.0000 - val_loss: 1486.2716 - val_mse: 2214544.7500 - 7s/epoch - 672ms/step\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 25: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 3277.0920 - mse: 21418036.0000 - val_loss: 2247.8564 - val_mse: 5059170.0000 - 6s/epoch - 548ms/step\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 26: val_loss did not improve from 77.40735\n",
            "11/11 - 8s - loss: 3010.3843 - mse: 17387306.0000 - val_loss: 5517.0439 - val_mse: 30447252.0000 - 8s/epoch - 702ms/step\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 27: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 2978.4907 - mse: 16765498.0000 - val_loss: 1577.9668 - val_mse: 2502645.2500 - 6s/epoch - 549ms/step\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 28: val_loss did not improve from 77.40735\n",
            "11/11 - 8s - loss: 2668.0762 - mse: 11152665.0000 - val_loss: 710.6911 - val_mse: 509838.5000 - 8s/epoch - 698ms/step\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 29: val_loss did not improve from 77.40735\n",
            "11/11 - 6s - loss: 2611.7417 - mse: 11146663.0000 - val_loss: 2616.4976 - val_mse: 6852744.5000 - 6s/epoch - 538ms/step\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 30: val_loss did not improve from 77.40735\n",
            "11/11 - 8s - loss: 2686.4514 - mse: 10227825.0000 - val_loss: 2049.0073 - val_mse: 4204542.0000 - 8s/epoch - 696ms/step\n",
            "Epoch 31/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 lstm + 코드의 깊이 증가 및 유닛의 수 증가로 바꿔주는 코드\n",
        "\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Bidirectional(LSTM(units=1024, activation='relu', return_sequences=True), input_shape=(trainX.shape[1], trainX.shape[2]))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=512, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=256, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=128, activation='relu',return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=64, activation='relu',return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=32, activation='relu',return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Bidirectional(LSTM(units=16, activation='relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Dense(units=1))\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "4h4WCyovjTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# Add a Convolutional layer\n",
        "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]))\n",
        "# model.add(MaxPooling1D(pool_size=2)\n",
        "\n",
        "# # Add LSTM layers\n",
        "# model.add(LSTM(units=1024, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=512, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=256, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=32, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=16, activation='relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "\n",
        "# # Add the output Dense layer\n",
        "# model.add(Dense(units=1))\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "-ggJjGXe977-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence 학습에 비교적 좋은 퍼포먼스를 내는 Huber()를 사용합니다.\n",
        "loss = Huber()\n",
        "optimizer = Adam(0.0005)\n",
        "model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
        "checkpoint = ModelCheckpoint(filename,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    validation_data=(test_data),\n",
        "                    epochs=50,\n",
        "                    callbacks=[checkpoint, earlystopping])\n",
        "\n",
        "model.load_weights(filename)"
      ],
      "metadata": {
        "id": "ec-zIuXPKP9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cT6pyykGN496"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}